# F1 PitWall AI - CI/CD Pipeline
# 
# This workflow runs on all pull requests and pushes to main.
# It includes:
# - Code quality checks (linting, formatting)
# - Unit and integration tests
# - Test coverage reporting
# - Dependency security scanning

name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.13'

jobs:
  # ============== LINTING & CODE QUALITY ==============
  lint:
    name: Code Quality
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install linting tools
        run: |
          python -m pip install --upgrade pip
          pip install ruff black isort mypy
      
      - name: Run Ruff (Fast Python Linter)
        run: |
          ruff check . --output-format=github
        continue-on-error: true
      
      - name: Check Black formatting
        run: |
          black --check --diff .
        continue-on-error: true
      
      - name: Check import sorting
        run: |
          isort --check-only --diff .
        continue-on-error: true

  # ============== UNIT TESTS ==============
  test-unit:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-xdist
      
      - name: Create cache directories
        run: |
          mkdir -p f1_cache f1_cache_dynasty cache
      
      - name: Run Unit Tests
        run: |
          pytest tests/ -m "unit or not (integration or slow or e2e)" \
            --cov=models --cov=utils --cov=data \
            --cov-report=xml \
            --cov-report=term-missing \
            -v --tb=short
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL || 'https://mock.supabase.co' }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY || 'mock-key' }}
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
        continue-on-error: true

  # ============== INTEGRATION TESTS ==============
  test-integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: test-unit
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Create cache directories
        run: |
          mkdir -p f1_cache f1_cache_dynasty cache
      
      - name: Run Integration Tests
        run: |
          pytest tests/ -m "integration" \
            --cov=models --cov=utils --cov=data \
            --cov-report=xml \
            -v --tb=short
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        continue-on-error: true
      
      - name: Upload integration coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: integration
        continue-on-error: true

  # ============== SECURITY SCANNING ==============
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install safety bandit
      
      - name: Check dependencies for vulnerabilities
        run: |
          pip install -r requirements.txt
          safety check --full-report
        continue-on-error: true
      
      - name: Run Bandit security linter
        run: |
          bandit -r models/ utils/ data/ -ll
        continue-on-error: true

  # ============== MODEL VALIDATION ==============
  validate-models:
    name: Model Validation
    runs-on: ubuntu-latest
    needs: test-unit
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create cache directories
        run: |
          mkdir -p f1_cache f1_cache_dynasty cache models/saved models/saved/hybrid
      
      - name: Run Model Validation Tests
        run: |
          python -c "
          from models.dynasty_engine import DynastyEngine, EloTracker, get_track_dna
          from models.hybrid_predictor import HybridPredictor
          
          # Test DynastyEngine components
          tracker = EloTracker()
          assert tracker.get_rating('VER') == 1500, 'Default Elo should be 1500'
          
          dna = get_track_dna('Monaco Grand Prix')
          assert dna['Type'] == 'Street_Slow', 'Monaco should be Street_Slow'
          assert dna['Overtaking'] == 1, 'Monaco overtaking should be 1'
          
          print('âœ… Model validation passed')
          "
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL || 'https://mock.supabase.co' }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY || 'mock-key' }}

  # ============== BUILD CHECK ==============
  build:
    name: Build Check
    runs-on: ubuntu-latest
    needs: [test-unit, security]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Verify imports
        run: |
          python -c "
          import sys
          sys.path.insert(0, '.')
          
          # Test critical imports
          from utils.exceptions import F1BaseException, DataError, ModelError
          from utils.db import get_supabase_client
          from models.dynasty_engine import DynastyEngine
          
          print('âœ… All critical imports successful')
          "
      
      - name: Check Streamlit app syntax
        run: |
          python -m py_compile app/main.py
          python -m py_compile app/pages/1_Season_Central.py
          python -m py_compile app/pages/3_predictions.py
          echo "âœ… Streamlit app syntax check passed"

  # ============== DEPLOYMENT (Streamlit Cloud) ==============
  deploy:
    name: Deploy to Streamlit Cloud
    runs-on: ubuntu-latest
    needs: [build, validate-models]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Notify deployment
        run: |
          echo "ðŸš€ Deployment triggered for Streamlit Cloud"
          echo "Streamlit Cloud will auto-deploy from main branch"
          echo "Check: https://share.streamlit.io/ for status"
