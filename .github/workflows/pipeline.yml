# F1 PitWall AI - Automated Data Pipeline
# 
# This workflow runs on a schedule to:
# 1. Check for new F1 race data
# 2. Ingest new race/qualifying results
# 3. Retrain ML models with latest data
# 4. Log pipeline status
#
# Schedule: Runs every 6 hours, and on-demand via workflow_dispatch
# Ideal timing: After race weekends (Sunday evenings) - cron runs at 00:00, 06:00, 12:00, 18:00 UTC

name: Automated Data Pipeline

on:
  # Scheduled runs - every 6 hours
  schedule:
    - cron: '0 0,6,12,18 * * *'  # At minute 0 of hours 0, 6, 12, 18 UTC
  
  # Manual trigger with options
  workflow_dispatch:
    inputs:
      force_retrain:
        description: 'Force model retraining even if no new data'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      years_to_ingest:
        description: 'Years to check for ingestion (comma-separated, e.g., 2024,2025)'
        required: false
        default: '2025'

env:
  PYTHON_VERSION: '3.10'

jobs:
  pipeline:
    name: Run Data Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create cache directories
        run: |
          mkdir -p f1_cache f1_cache_dynasty cache models/saved models/saved/hybrid
      
      - name: Run Pipeline Orchestrator
        id: pipeline
        run: |
          echo "ðŸš€ Starting Automated Pipeline..."
          echo "Time: $(date -u)"
          echo "---"
          
          python -c "
          import sys
          sys.path.insert(0, '.')
          
          from pipelines.orchestrator import run_pipeline
          from utils.logger import get_logger
          
          logger = get_logger('GHActions-Pipeline')
          
          try:
              logger.info('GitHub Actions Pipeline triggered.')
              run_pipeline()
              logger.info('Pipeline completed successfully.')
              print('::notice::Pipeline completed successfully')
          except Exception as e:
              logger.error(f'Pipeline failed: {e}')
              print(f'::error::Pipeline failed: {e}')
              raise
          "
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      
      - name: Force Retrain (if requested)
        if: ${{ github.event.inputs.force_retrain == 'true' }}
        run: |
          echo "ðŸ”„ Force retraining models..."
          python -c "
          import sys
          sys.path.insert(0, '.')
          
          from models.hybrid_predictor import HybridPredictor
          
          predictor = HybridPredictor()
          predictor.train()
          print('âœ… Force retrain complete')
          "
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      
      - name: Check Database Status
        run: |
          python -c "
          import sys
          sys.path.insert(0, '.')
          from utils.db import get_supabase_client
          
          s = get_supabase_client()
          
          # Count records
          races = s.table('races').select('count', count='exact').execute()
          complete = s.table('races').select('count', count='exact').eq('ingestion_status', 'COMPLETE').execute()
          laps = s.table('laps').select('count', count='exact').execute()
          results = s.table('race_results').select('count', count='exact').execute()
          
          print('ðŸ“Š Database Status:')
          print(f'  Total Races:     {races.count}')
          print(f'  Complete Races:  {complete.count}')
          print(f'  Total Laps:      {laps.count}')
          print(f'  Race Results:    {results.count}')
          "
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      
      - name: Upload Pipeline Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-${{ github.run_id }}
          path: |
            logs/
            mlruns/
          retention-days: 14
        continue-on-error: true

  notify:
    name: Notify on Failure
    runs-on: ubuntu-latest
    needs: pipeline
    if: failure()
    
    steps:
      - name: Log Failure
        run: |
          echo "::error::Automated pipeline failed! Check the logs."
          echo "Failed at: $(date -u)"
